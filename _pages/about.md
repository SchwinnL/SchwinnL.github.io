---
permalink: /
title: "**Leo** Schwinn"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
news: true
---

I'm a Lecturer at the Technical University of Munich, supervising and co-supervising 4 PhD students. I am interested in robust machine learning with a focus on data-efficient learning and robustness vulnerabilities of Large Language Models (LLMs). I am currently the main organizer of the ICLR BlogPost Track and am always happy to answer questions about the track. 

Previously, I did my Ph.D. at Friedrich-Alexander University Erlangen-NÃ¼rnberg (FAU) under the supervision of Prof. BjÃ¶rn Eskofier, receiving the [ATE dissertation price](https://www.tf.fau.de/2024/11/allgemein/verleihung-der-promotionspreise-2024/). During my PhD I conducted an internship under the supervision of [Prof. Doina Precup at the Mila Quebec AI Institute](https://mila.quebec/en/person/leo-schwinn/). After my PhD, I worked as a Postdoc at FAU and as a machine learning engineer at Robert Bosch on robust computer vision for autonomous driving. 

### News

- January, 2026: **2** AI safety papers accepted at **ICLR 2026**, Converting a bug into a feature by using model collapse as a reliable machine unlearning technique for LLMs in [Model Collapse Is Not a Bug but a Feature in Machine Unlearning for LLMs
](https://arxiv.org/abs/2507.04219) and showcasing the importance of probabilistic evaluations and sampling in adversarial attacks in LLMs [Sampling-aware Adversarial Attacks Against Large Language Models](https://arxiv.org/abs/2507.04446)

- September, 2025: **2** papers accepted at **NeurIPS 2025**, We capture complex inter-table dependencies in graph generation tasks [Joint Relational Database Generation via Graph-Conditional Diffusion Models
](https://arxiv.org/abs/2505.16527), and we make Vision Language Models work for high-detail VQA tasks in [FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual Question Answering
](https://arxiv.org/abs/2506.21710)

- May, 2025: We are presenting **2** papers at **ICML 2025**, How to make your time-series transformer and state-space model efficient: [Efficient Time Series Processing for Transformers and State-Space Models through Token Merging
](https://arxiv.org/abs/2405.17951), and we formalize the question "When to retrain a machine learning model" in real-world environments where new data is constantly arriving and changing.


- February, 2025: We are presenting **1** paper at **CVPR 2025** :-), In [Joint Out-of-Distribution Filtering and Data Discovery Active Learning](https://arxiv.org/abs/2503.02491), we introduce Joda, the first active learning approach to handle both out-of-distribution data and new category discovery at the same time.

- February, 2025: A [probabilistic perspective into unlearning in LLMs](https://arxiv.org/pdf/2410.03523) was awarded an **Oral presentation at ICLR 2025** ðŸ¥³

- January, 2025: We are presenting **2** papers at **ICLR 2025**! A [probabilistic perspective into unlearning in LLMs](https://arxiv.org/pdf/2410.03523), where we reveal that current evaluations are unreliable. Moreover, we introduce a novel probabilistic time series forcasting algorithm [combining Gaussian Process priors and flow matching](https://arxiv.org/pdf/2410.03024)

- September, 2024: Happy to have **3** papers accepted at **NeurIPS 2024**!, we [explore new threat models in LLMs](https://arxiv.org/abs/2402.09063), propose the first [efficient adversarial training algorithm for LLMs](https://arxiv.org/abs/2405.15589) presented as spotlight ðŸ¥³, and [explore scaling properties of Lipschitz-1 neural networks](https://arxiv.org/abs/2305.10388)

- June, 2024: Position is filled! <s>Open PhD position available at my research group at the chair of Prof. Stephan GÃ¼nnemann! For more information, see [here](/files/E13_Geomar.pdf)</s>

- December, 2023: I am one of the review process chairs of the [2024 Conference on Lifelong Learning Agents (CoLLAs)](https://lifelong-ml.cc/Conferences/2024/dates). Looking forward to see everyone in Pisa!

- November, 2023: I will start as a Postdoc at the [Technical University of Munich](https://www.tum.de/) and [Munich Data Science Institute](https://www.mdsi.tum.de/mdsi/startseite/) in the [Data Analytics and Machine Learning group](https://www.cs.cit.tum.de/daml/startseite/) supervised by Prof. Stephan GÃ¼nnemann

- October, 2023: We will present ["Adversarial Attacks and Defenses in Large Language Models: Old and New Threats"](https://arxiv.org/abs/2310.19737) as a **spotlight** presentation at NeurIPS 2023 I Canâ€™t Believe Itâ€™s Not Better (ICBINB): Failure Modes in the Age of Foundation Models Workshop.

- October, 2023: I'm now a member of [ELLIS - the European Laboratory for Learning and Intelligent Systems](https://ellis.eu/)

- April, 2023: I am an organizer / track chair of the [ICLR Blogpost track 2024](https://iclr-blogposts.github.io/2024/about)
